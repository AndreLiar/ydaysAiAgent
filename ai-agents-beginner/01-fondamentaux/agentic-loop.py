#!/usr/bin/env python3
"""
Fondamentaux - Boucle Agentique Universelle
Impl√©mentation du pattern Perception ‚Üí Plan ‚Üí Act ‚Üí Reflect
"""

import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

class BaseAgent:
    """
    Agent de base impl√©mentant la boucle agentique universelle
    Perception ‚Üí Plan ‚Üí Act ‚Üí Reflect
    """
    
    def __init__(self, role: str = "Assistant", model: str = "gpt-4"):
        self.role = role
        self.model = model
        self.client = OpenAI()
        self.execution_history = []
        self.stats = {
            "total_runs": 0,
            "successful_runs": 0,
            "average_reflection_score": 0.0
        }
    
    def perceive(self, input_data: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        PERCEPTION: Analyser l'input utilisateur et le contexte
        """
        print("üîç PERCEPTION - Analyse de l'input...")
        
        perception = {
            "user_input": input_data,
            "timestamp": datetime.now().isoformat(),
            "context": context or {},
            "input_type": self._classify_input(input_data),
            "complexity": self._estimate_complexity(input_data),
            "role": self.role
        }
        
        print(f"   Type: {perception['input_type']}")
        print(f"   Complexit√©: {perception['complexity']}/5")
        
        return perception
    
    def plan(self, perception: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        PLAN: Cr√©er un plan d'action bas√© sur la perception
        """
        print("üìã PLAN - Cr√©ation du plan d'action...")
        
        planning_prompt = f"""
        Tu es un {self.role}. Analyse cette demande et cr√©e un plan d'action.
        
        Demande: {perception['user_input']}
        Type: {perception['input_type']}
        Complexit√©: {perception['complexity']}/5
        
        R√©ponds UNIQUEMENT avec un JSON contenant:
        {{
            "steps": [
                {{"action": "action_name", "description": "what to do", "priority": 1}},
                ...
            ],
            "estimated_time": "temps estim√©",
            "approach": "approche choisie"
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": planning_prompt}],
                temperature=0.3
            )
            
            plan_json = json.loads(response.choices[0].message.content)
            
            print(f"   Approche: {plan_json.get('approach', 'Standard')}")
            print(f"   √âtapes: {len(plan_json.get('steps', []))}")
            print(f"   Temps estim√©: {plan_json.get('estimated_time', 'Non estim√©')}")
            
            return plan_json.get('steps', [])
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur de planification: {e}")
            # Plan de fallback
            return [{"action": "respond", "description": "R√©pondre directement", "priority": 1}]
    
    def act(self, plan: List[Dict[str, Any]], perception: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ACT: Ex√©cuter chaque √©tape du plan
        """
        print("‚ö° ACTION - Ex√©cution du plan...")
        
        results = []
        
        for i, step in enumerate(plan, 1):
            print(f"   √âtape {i}: {step.get('description', step.get('action'))}")
            
            if step.get('action') == 'respond':
                # Action principale: g√©n√©rer une r√©ponse
                response = self._generate_response(perception['user_input'])
                results.append({
                    "step": i,
                    "action": step.get('action'),
                    "result": response,
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
                
            elif step.get('action') == 'search':
                # Simulation d'une recherche
                results.append({
                    "step": i,
                    "action": step.get('action'),
                    "result": f"Recherche effectu√©e pour: {perception['user_input'][:50]}...",
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
                
            else:
                # Action g√©n√©rique
                results.append({
                    "step": i,
                    "action": step.get('action'),
                    "result": f"Action '{step.get('action')}' ex√©cut√©e",
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
        
        print(f"   ‚úÖ {len(results)} actions ex√©cut√©es")
        return results
    
    def reflect(self, results: List[Dict[str, Any]], expected_outcome: Optional[str] = None) -> Dict[str, Any]:
        """
        REFLECT: √âvaluer les r√©sultats et identifier les am√©liorations
        """
        print("ü§î REFLECTION - √âvaluation des r√©sultats...")
        
        # Calcul des m√©triques de base
        successful_steps = sum(1 for r in results if r.get('success', False))
        total_steps = len(results)
        success_rate = successful_steps / total_steps if total_steps > 0 else 0
        
        # Auto-√©valuation avec le LLM
        reflection_prompt = f"""
        √âvalue cette ex√©cution d'agent sur une √©chelle de 1-5:
        
        R√©sultats: {json.dumps(results, indent=2)}
        Objectif attendu: {expected_outcome or "Non sp√©cifi√©"}
        
        R√©ponds UNIQUEMENT avec un JSON:
        {{
            "quality_score": 4.2,
            "strengths": ["point fort 1", "point fort 2"],
            "improvements": ["am√©lioration 1", "am√©lioration 2"],
            "confidence": 0.85,
            "next_actions": ["action 1", "action 2"]
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": reflection_prompt}],
                temperature=0.2
            )
            
            auto_reflection = json.loads(response.choices[0].message.content)
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur d'auto-r√©flection: {e}")
            auto_reflection = {
                "quality_score": 3.0,
                "strengths": ["Ex√©cution compl√®te"],
                "improvements": ["Gestion d'erreurs"],
                "confidence": 0.5,
                "next_actions": ["Am√©liorer la robustesse"]
            }
        
        reflection = {
            "success_rate": success_rate,
            "total_steps": total_steps,
            "quality_score": auto_reflection.get("quality_score", 3.0),
            "strengths": auto_reflection.get("strengths", []),
            "improvements": auto_reflection.get("improvements", []),
            "confidence": auto_reflection.get("confidence", 0.5),
            "next_actions": auto_reflection.get("next_actions", []),
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"   Score qualit√©: {reflection['quality_score']}/5")
        print(f"   Taux de succ√®s: {reflection['success_rate']*100:.1f}%")
        print(f"   Points forts: {', '.join(reflection['strengths'][:2])}")
        
        return reflection
    
    def run_loop(self, input_data: str, expected_outcome: Optional[str] = None) -> Dict[str, Any]:
        """
        Ex√©cuter la boucle agentique compl√®te
        Perception ‚Üí Plan ‚Üí Act ‚Üí Reflect
        """
        print(f"ü§ñ D√©marrage de la boucle agentique - {self.role}")
        print("=" * 60)
        
        # 1. PERCEPTION
        perception = self.perceive(input_data)
        
        # 2. PLAN
        plan = self.plan(perception)
        
        # 3. ACT
        results = self.act(plan, perception)
        
        # 4. REFLECT
        reflection = self.reflect(results, expected_outcome)
        
        # Enregistrer l'ex√©cution
        execution = {
            "input": input_data,
            "perception": perception,
            "plan": plan,
            "results": results,
            "reflection": reflection,
            "final_output": results[-1].get('result') if results else "Aucun r√©sultat"
        }
        
        self.execution_history.append(execution)
        
        # Mettre √† jour les statistiques
        self._update_stats(reflection)
        
        print("=" * 60)
        print(f"‚úÖ Boucle termin√©e - Score: {reflection['quality_score']}/5")
        
        return execution
    
    def _classify_input(self, input_data: str) -> str:
        """Classifier le type d'input"""
        if "?" in input_data:
            return "question"
        elif any(word in input_data.lower() for word in ["cr√©er", "g√©n√©rer", "faire", "impl√©menter"]):
            return "creation"
        elif any(word in input_data.lower() for word in ["analyser", "examiner", "√©tudier"]):
            return "analysis"
        else:
            return "general"
    
    def _estimate_complexity(self, input_data: str) -> int:
        """Estimer la complexit√© sur 5"""
        words = len(input_data.split())
        if words < 10:
            return 1
        elif words < 20:
            return 2
        elif words < 40:
            return 3
        elif words < 80:
            return 4
        else:
            return 5
    
    def _generate_response(self, input_data: str) -> str:
        """G√©n√©rer une r√©ponse avec le LLM"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"Tu es un {self.role} expert et utile."},
                    {"role": "user", "content": input_data}
                ],
                temperature=0.7,
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Erreur lors de la g√©n√©ration: {e}"
    
    def _update_stats(self, reflection: Dict[str, Any]):
        """Mettre √† jour les statistiques de performance"""
        self.stats["total_runs"] += 1
        if reflection["quality_score"] >= 3.0:
            self.stats["successful_runs"] += 1
        
        # Moyenne mobile du score de r√©flection
        current_avg = self.stats["average_reflection_score"]
        new_score = reflection["quality_score"]
        n = self.stats["total_runs"]
        self.stats["average_reflection_score"] = (current_avg * (n-1) + new_score) / n
    
    def get_stats(self) -> Dict[str, Any]:
        """Obtenir les statistiques de performance"""
        return self.stats.copy()
    
    def get_execution_history(self) -> List[Dict[str, Any]]:
        """Obtenir l'historique des ex√©cutions"""
        return self.execution_history.copy()


def demo_agentic_loop():
    """D√©monstration de la boucle agentique"""
    
    print("üöÄ D√©monstration de la Boucle Agentique")
    print("=" * 60)
    
    # Cr√©er un agent
    agent = BaseAgent(role="Expert en IA", model="gpt-4")
    
    # Test 1: Question simple
    print("\nüìù Test 1: Question Simple")
    result1 = agent.run_loop(
        "Qu'est-ce qu'un agent IA?",
        expected_outcome="D√©finition claire et concise d'un agent IA"
    )
    
    # Test 2: T√¢che complexe
    print("\nüìù Test 2: T√¢che de Cr√©ation")
    result2 = agent.run_loop(
        "Cr√©er un plan de d√©veloppement pour une application mobile de e-commerce avec des agents IA int√©gr√©s",
        expected_outcome="Plan d√©taill√© avec √©tapes, technologies et timeline"
    )
    
    # Test 3: Analyse
    print("\nüìù Test 3: T√¢che d'Analyse")
    result3 = agent.run_loop(
        "Analyser les avantages et inconv√©nients des diff√©rents frameworks d'agents IA: LangChain, AutoGen, CrewAI",
        expected_outcome="Comparaison d√©taill√©e avec recommandations"
    )
    
    # Afficher les statistiques
    print("\nüìä Statistiques de Performance")
    stats = agent.get_stats()
    print(f"   Ex√©cutions totales: {stats['total_runs']}")
    print(f"   Ex√©cutions r√©ussies: {stats['successful_runs']}")
    print(f"   Taux de succ√®s: {stats['successful_runs']/stats['total_runs']*100:.1f}%")
    print(f"   Score moyen: {stats['average_reflection_score']:.2f}/5")
    
    return agent


if __name__ == "__main__":
    # V√©rifier la cl√© API
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è  OPENAI_API_KEY manquante!")
        print("   Ajoutez votre cl√© dans le fichier .env")
        print("   Ou utilisez: export OPENAI_API_KEY='your-key-here'")
        exit(1)
    
    # Lancer la d√©monstration
    agent = demo_agentic_loop()
    
    # Mode interactif
    print("\nüéÆ Mode Interactif - Testez votre agent!")
    print("   Tapez 'quit' pour quitter")
    
    while True:
        user_input = input("\nüí¨ Votre question/t√¢che: ")
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            break
        
        if user_input.strip():
            result = agent.run_loop(user_input)
            print(f"\nüéØ R√©ponse finale:")
            print(f"   {result['final_output']}")
    
    print("\nüëã Merci d'avoir test√© la boucle agentique!")
    print(f"   Statistiques finales: {agent.get_stats()}")