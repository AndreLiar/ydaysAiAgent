#!/usr/bin/env python3
"""
Fondamentaux - Boucle Agentique Universelle
ImplÃ©mentation du pattern Perception â†’ Plan â†’ Act â†’ Reflect
"""

import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

class BaseAgent:
    """
    Agent de base implÃ©mentant la boucle agentique universelle
    Perception â†’ Plan â†’ Act â†’ Reflect
    """
    
    def __init__(self, role: str = "Assistant", model: str = "gpt-4"):
        self.role = role
        self.model = model
        self.client = OpenAI()
        self.execution_history = []
        self.stats = {
            "total_runs": 0,
            "successful_runs": 0,
            "average_reflection_score": 0.0
        }
    
    def perceive(self, input_data: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        PERCEPTION: Analyser l'input utilisateur et le contexte
        """
        print("ğŸ” PERCEPTION - Analyse de l'input...")
        
        perception = {
            "user_input": input_data,
            "timestamp": datetime.now().isoformat(),
            "context": context or {},
            "input_type": self._classify_input(input_data),
            "complexity": self._estimate_complexity(input_data),
            "role": self.role
        }
        
        print(f"   Type: {perception['input_type']}")
        print(f"   ComplexitÃ©: {perception['complexity']}/5")
        
        return perception
    
    def plan(self, perception: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        PLAN: CrÃ©er un plan d'action basÃ© sur la perception
        """
        print("ğŸ“‹ PLAN - CrÃ©ation du plan d'action...")
        
        planning_prompt = f"""
        Tu es un {self.role}. Analyse cette demande et crÃ©e un plan d'action.
        
        Demande: {perception['user_input']}
        Type: {perception['input_type']}
        ComplexitÃ©: {perception['complexity']}/5
        
        RÃ©ponds UNIQUEMENT avec un JSON contenant:
        {{
            "steps": [
                {{"action": "action_name", "description": "what to do", "priority": 1}},
                ...
            ],
            "estimated_time": "temps estimÃ©",
            "approach": "approche choisie"
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": planning_prompt}],
                temperature=0.3
            )
            
            plan_json = json.loads(response.choices[0].message.content)
            
            print(f"   Approche: {plan_json.get('approach', 'Standard')}")
            print(f"   Ã‰tapes: {len(plan_json.get('steps', []))}")
            print(f"   Temps estimÃ©: {plan_json.get('estimated_time', 'Non estimÃ©')}")
            
            return plan_json.get('steps', [])
            
        except Exception as e:
            print(f"   âš ï¸  Erreur de planification: {e}")
            # Plan de fallback
            return [{"action": "respond", "description": "RÃ©pondre directement", "priority": 1}]
    
    def act(self, plan: List[Dict[str, Any]], perception: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ACT: ExÃ©cuter chaque Ã©tape du plan
        """
        print("âš¡ ACTION - ExÃ©cution du plan...")
        
        results = []
        
        for i, step in enumerate(plan, 1):
            print(f"   Ã‰tape {i}: {step.get('description', step.get('action'))}")
            
            if step.get('action') == 'respond':
                # Action principale: gÃ©nÃ©rer une rÃ©ponse
                response = self._generate_response(perception['user_input'])
                results.append({
                    "step": i,
                    "action": step.get('action'),
                    "result": response,
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
                
            elif step.get('action') == 'search':
                # Simulation d'une recherche
                results.append({
                    "step": i,
                    "action": step.get('action'),
                    "result": f"Recherche effectuÃ©e pour: {perception['user_input'][:50]}...",
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
                
            else:
                # Action gÃ©nÃ©rique
                results.append({
                    "step": i,
                    "action": step.get('action'),
                    "result": f"Action '{step.get('action')}' exÃ©cutÃ©e",
                    "success": True,
                    "timestamp": datetime.now().isoformat()
                })
        
        print(f"   âœ… {len(results)} actions exÃ©cutÃ©es")
        return results
    
    def reflect(self, results: List[Dict[str, Any]], expected_outcome: Optional[str] = None) -> Dict[str, Any]:
        """
        REFLECT: Ã‰valuer les rÃ©sultats et identifier les amÃ©liorations
        """
        print("ğŸ¤” REFLECTION - Ã‰valuation des rÃ©sultats...")
        
        # Calcul des mÃ©triques de base
        successful_steps = sum(1 for r in results if r.get('success', False))
        total_steps = len(results)
        success_rate = successful_steps / total_steps if total_steps > 0 else 0
        
        # Auto-Ã©valuation avec le LLM
        reflection_prompt = f"""
        Ã‰value cette exÃ©cution d'agent sur une Ã©chelle de 1-5:
        
        RÃ©sultats: {json.dumps(results, indent=2)}
        Objectif attendu: {expected_outcome or "Non spÃ©cifiÃ©"}
        
        RÃ©ponds UNIQUEMENT avec un JSON:
        {{
            "quality_score": 4.2,
            "strengths": ["point fort 1", "point fort 2"],
            "improvements": ["amÃ©lioration 1", "amÃ©lioration 2"],
            "confidence": 0.85,
            "next_actions": ["action 1", "action 2"]
        }}
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": reflection_prompt}],
                temperature=0.2
            )
            
            auto_reflection = json.loads(response.choices[0].message.content)
            
        except Exception as e:
            print(f"   âš ï¸  Erreur d'auto-rÃ©flection: {e}")
            auto_reflection = {
                "quality_score": 3.0,
                "strengths": ["ExÃ©cution complÃ¨te"],
                "improvements": ["Gestion d'erreurs"],
                "confidence": 0.5,
                "next_actions": ["AmÃ©liorer la robustesse"]
            }
        
        reflection = {
            "success_rate": success_rate,
            "total_steps": total_steps,
            "quality_score": auto_reflection.get("quality_score", 3.0),
            "strengths": auto_reflection.get("strengths", []),
            "improvements": auto_reflection.get("improvements", []),
            "confidence": auto_reflection.get("confidence", 0.5),
            "next_actions": auto_reflection.get("next_actions", []),
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"   Score qualitÃ©: {reflection['quality_score']}/5")
        print(f"   Taux de succÃ¨s: {reflection['success_rate']*100:.1f}%")
        print(f"   Points forts: {', '.join(reflection['strengths'][:2])}")
        
        return reflection
    
    def run_loop(self, input_data: str, expected_outcome: Optional[str] = None) -> Dict[str, Any]:
        """
        ExÃ©cuter la boucle agentique complÃ¨te
        Perception â†’ Plan â†’ Act â†’ Reflect
        """
        print(f"ğŸ¤– DÃ©marrage de la boucle agentique - {self.role}")
        print("=" * 60)
        
        # 1. PERCEPTION
        perception = self.perceive(input_data)
        
        # 2. PLAN
        plan = self.plan(perception)
        
        # 3. ACT
        results = self.act(plan, perception)
        
        # 4. REFLECT
        reflection = self.reflect(results, expected_outcome)
        
        # Enregistrer l'exÃ©cution
        execution = {
            "input": input_data,
            "perception": perception,
            "plan": plan,
            "results": results,
            "reflection": reflection,
            "final_output": results[-1].get('result') if results else "Aucun rÃ©sultat"
        }
        
        self.execution_history.append(execution)
        
        # Mettre Ã  jour les statistiques
        self._update_stats(reflection)
        
        print("=" * 60)
        print(f"âœ… Boucle terminÃ©e - Score: {reflection['quality_score']}/5")
        
        return execution
    
    def _classify_input(self, input_data: str) -> str:
        """Classifier le type d'input"""
        if "?" in input_data:
            return "question"
        elif any(word in input_data.lower() for word in ["crÃ©er", "gÃ©nÃ©rer", "faire", "implÃ©menter"]):
            return "creation"
        elif any(word in input_data.lower() for word in ["analyser", "examiner", "Ã©tudier"]):
            return "analysis"
        else:
            return "general"
    
    def _estimate_complexity(self, input_data: str) -> int:
        """Estimer la complexitÃ© sur 5"""
        words = len(input_data.split())
        if words < 10:
            return 1
        elif words < 20:
            return 2
        elif words < 40:
            return 3
        elif words < 80:
            return 4
        else:
            return 5
    
    def _generate_response(self, input_data: str) -> str:
        """GÃ©nÃ©rer une rÃ©ponse avec le LLM"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"Tu es un {self.role} expert et utile."},
                    {"role": "user", "content": input_data}
                ],
                temperature=0.7,
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Erreur lors de la gÃ©nÃ©ration: {e}"
    
    def _update_stats(self, reflection: Dict[str, Any]):
        """Mettre Ã  jour les statistiques de performance"""
        self.stats["total_runs"] += 1
        if reflection["quality_score"] >= 3.0:
            self.stats["successful_runs"] += 1
        
        # Moyenne mobile du score de rÃ©flection
        current_avg = self.stats["average_reflection_score"]
        new_score = reflection["quality_score"]
        n = self.stats["total_runs"]
        self.stats["average_reflection_score"] = (current_avg * (n-1) + new_score) / n
    
    def get_stats(self) -> Dict[str, Any]:
        """Obtenir les statistiques de performance"""
        return self.stats.copy()
    
    def get_execution_history(self) -> List[Dict[str, Any]]:
        """Obtenir l'historique des exÃ©cutions"""
        return self.execution_history.copy()


def demo_agentic_loop():
    """DÃ©monstration de la boucle agentique"""
    
    print("ğŸš€ DÃ©monstration de la Boucle Agentique")
    print("=" * 60)
    
    # CrÃ©er un agent
    agent = BaseAgent(role="Expert en IA", model="gpt-4")
    
    # Test 1: Question simple
    print("\nğŸ“ Test 1: Question Simple")
    result1 = agent.run_loop(
        "Qu'est-ce qu'un agent IA?",
        expected_outcome="DÃ©finition claire et concise d'un agent IA"
    )
    
    # Test 2: TÃ¢che complexe
    print("\nğŸ“ Test 2: TÃ¢che de CrÃ©ation")
    result2 = agent.run_loop(
        "CrÃ©er un plan de dÃ©veloppement pour une application mobile de e-commerce avec des agents IA intÃ©grÃ©s",
        expected_outcome="Plan dÃ©taillÃ© avec Ã©tapes, technologies et timeline"
    )
    
    # Test 3: Analyse
    print("\nğŸ“ Test 3: TÃ¢che d'Analyse")
    result3 = agent.run_loop(
        "Analyser les avantages et inconvÃ©nients des diffÃ©rents frameworks d'agents IA: LangChain, AutoGen, CrewAI",
        expected_outcome="Comparaison dÃ©taillÃ©e avec recommandations"
    )
    
    # Afficher les statistiques
    print("\nğŸ“Š Statistiques de Performance")
    stats = agent.get_stats()
    print(f"   ExÃ©cutions totales: {stats['total_runs']}")
    print(f"   ExÃ©cutions rÃ©ussies: {stats['successful_runs']}")
    print(f"   Taux de succÃ¨s: {stats['successful_runs']/stats['total_runs']*100:.1f}%")
    print(f"   Score moyen: {stats['average_reflection_score']:.2f}/5")
    
    return agent


if __name__ == "__main__":
    # VÃ©rifier la clÃ© API
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEY manquante!")
        print("   Ajoutez votre clÃ© dans le fichier .env")
        print("   Ou utilisez: export OPENAI_API_KEY='your-key-here'")
        exit(1)
    
    # Lancer la dÃ©monstration
    agent = demo_agentic_loop()
    
    # Mode interactif
    print("\nğŸ® Mode Interactif - Testez votre agent!")
    print("   Tapez 'quit' pour quitter")
    
    while True:
        user_input = input("\nğŸ’¬ Votre question/tÃ¢che: ")
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            break
        
        if user_input.strip():
            result = agent.run_loop(user_input)
            print(f"\nğŸ¯ RÃ©ponse finale:")
            print(f"   {result['final_output']}")
    
    print("\nğŸ‘‹ Merci d'avoir testÃ© la boucle agentique!")
    print(f"   Statistiques finales: {agent.get_stats()}")