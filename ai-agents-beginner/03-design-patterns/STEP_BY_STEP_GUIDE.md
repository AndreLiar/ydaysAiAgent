# üéØ Guide √âtape par √âtape - Patterns Avanc√©s des Agents IA

## üìö Vue d'Ensemble du Projet

Ce guide vous accompagne dans la ma√Ætrise des **5 patterns avanc√©s** pour cr√©er des syst√®mes d'agents IA sophistiqu√©s. Vous apprendrez en faisant - chaque √©tape vous enseigne des concepts complexes tout en construisant un √©cosyst√®me d'agents collaboratifs.

### üéØ Objectifs d'Apprentissage
- Ma√Ætriser les patterns avanc√©s de conception d'agents
- Impl√©menter la collaboration multi-agents
- Cr√©er des syst√®mes auto-correcteurs
- Int√©grer RAG (Retrieval-Augmented Generation)
- D√©velopper la planification automatique avanc√©e
- Construire des architectures distribu√©es

### üí° Pr√©requis
- ‚úÖ Module 1 compl√©t√© (patterns de base : Single Agent, Tool Use, Human-in-Loop)
- ‚úÖ Connaissance de la boucle agentique
- ‚úÖ Exp√©rience avec LLM + Tools + Memory

## üöÄ D√©marrage Rapide

```bash
# 1. Installer les d√©pendances avanc√©es
pip install openai chromadb sentence-transformers langchain-community faiss-cpu

# 2. Configurer votre environnement
cp .env.example .env
# Ajouter votre OPENAI_API_KEY dans le fichier .env

# 3. Lancer le projet avanc√©
python my_advanced_agent_system_starter.py
```

## üìã Progression √âtape par √âtape

### ‚úÖ TODO 1: Architecture Avanc√©e et Setup (5 min)

**Concepts appris**: Architecture multi-agents et d√©pendances avanc√©es

```bash
pip install openai chromadb sentence-transformers langchain-community faiss-cpu networkx
```

**Pourquoi ces packages ?**
- `chromadb`: Base de donn√©es vectorielle pour RAG
- `sentence-transformers`: Embeddings s√©mantiques
- `langchain-community`: Outils communautaires LangChain  
- `faiss-cpu`: Recherche vectorielle haute performance
- `networkx`: Gestion de graphes pour multi-agents

Impl√©mentez l'architecture de base:

```python
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable, Union
from datetime import datetime
from enum import Enum
import json
import asyncio
import uuid

class AgentRole(Enum):
    """R√¥les des agents dans le syst√®me"""
    ORCHESTRATOR = "orchestrator"
    SPECIALIST = "specialist"
    VALIDATOR = "validator"
    RESEARCHER = "researcher"
    PLANNER = "planner"
    EXECUTOR = "executor"

class PatternType(Enum):
    """Types de patterns avanc√©s"""
    MULTI_AGENT_COLLAB = "multi_agent_collaboration"
    SELF_CORRECTION = "self_correction"
    RAG_AGENT = "rag_agent"
    PLANNING_AGENT = "planning_agent"
    MULTI_AGENT_SYSTEM = "multi_agent_system"

@dataclass
class AgentMessage:
    """Message standardis√© entre agents"""
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    sender_id: str = ""
    recipient_id: str = ""
    content: str = ""
    message_type: str = "communication"
    priority: int = 1
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class AgentState:
    """√âtat avanc√© d'un agent"""
    agent_id: str
    role: AgentRole
    capabilities: List[str]
    knowledge_base: Dict[str, Any] = field(default_factory=dict)
    conversation_history: List[Dict] = field(default_factory=list)
    collaboration_history: List[AgentMessage] = field(default_factory=list)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    current_tasks: List[Dict] = field(default_factory=list)
    learning_insights: List[str] = field(default_factory=list)

class AdvancedAgentBase:
    """Classe de base pour tous les agents avanc√©s"""
    
    def __init__(self, agent_id: str, role: AgentRole, capabilities: List[str]):
        self.state = AgentState(
            agent_id=agent_id,
            role=role,
            capabilities=capabilities
        )
        self.message_queue = asyncio.Queue()
        self.collaboration_network = {}
        self.is_active = True
        
        print(f"ü§ñ Agent {agent_id} initialis√© - R√¥le: {role.value}")
        print(f"üéØ Capacit√©s: {', '.join(capabilities)}")
    
    async def send_message(self, recipient_id: str, content: str, message_type: str = "communication") -> bool:
        """Envoyer un message √† un autre agent"""
        message = AgentMessage(
            sender_id=self.state.agent_id,
            recipient_id=recipient_id,
            content=content,
            message_type=message_type
        )
        
        self.state.collaboration_history.append(message)
        
        # En production: vraie messagerie inter-agents
        print(f"üì§ {self.state.agent_id} ‚Üí {recipient_id}: {content[:50]}...")
        return True
    
    async def receive_message(self, message: AgentMessage) -> Dict[str, Any]:
        """Recevoir et traiter un message"""
        print(f"üì• {self.state.agent_id} re√ßoit de {message.sender_id}: {message.content[:50]}...")
        
        await self.message_queue.put(message)
        return await self.process_message(message)
    
    async def process_message(self, message: AgentMessage) -> Dict[str, Any]:
        """Traiter un message re√ßu - √† impl√©menter par les sous-classes"""
        return {"processed": True, "response": "Message trait√©"}
    
    def update_performance_metric(self, metric_name: str, value: float):
        """Mettre √† jour une m√©trique de performance"""
        self.state.performance_metrics[metric_name] = value
        
    def add_learning_insight(self, insight: str):
        """Ajouter un insight d'apprentissage"""
        self.state.learning_insights.append({
            "insight": insight,
            "timestamp": datetime.now().isoformat()
        })
```

### ‚úÖ TODO 2: Pattern 1 - Multi-Agent Collaboration (20 min)

**Concepts appris**: Orchestration et sp√©cialisation d'agents

Impl√©mentez le syst√®me de collaboration multi-agents:

```python
class OrchestratorAgent(AdvancedAgentBase):
    """Agent orchestrateur pour la collaboration multi-agents"""
    
    def __init__(self):
        super().__init__(
            agent_id="orchestrator_001",
            role=AgentRole.ORCHESTRATOR,
            capabilities=["coordination", "task_distribution", "result_synthesis"]
        )
        self.specialist_agents = {}
        self.current_workflow = None
        
    def register_specialist(self, specialist_agent):
        """Enregistrer un agent sp√©cialiste"""
        self.specialist_agents[specialist_agent.state.agent_id] = specialist_agent
        print(f"‚úÖ Sp√©cialiste enregistr√©: {specialist_agent.state.agent_id} ({specialist_agent.state.role.value})")
    
    async def coordinate_task(self, task: str) -> Dict[str, Any]:
        """Coordonner une t√¢che complexe entre sp√©cialistes"""
        print(f"\nüéØ COORDINATION MULTI-AGENTS: {task}")
        print("=" * 60)
        
        # Analyser la t√¢che et identifier les sp√©cialistes n√©cessaires
        required_specialists = self._identify_required_specialists(task)
        
        # Cr√©er un workflow de collaboration
        workflow = await self._create_collaboration_workflow(task, required_specialists)
        self.current_workflow = workflow
        
        # Ex√©cuter le workflow
        results = await self._execute_workflow(workflow)
        
        # Synth√©tiser les r√©sultats
        final_result = await self._synthesize_results(task, results)
        
        return {
            "task": task,
            "specialists_involved": list(required_specialists.keys()),
            "workflow_steps": len(workflow["steps"]),
            "individual_results": results,
            "synthesized_result": final_result,
            "success": final_result.get("success", False),
            "collaboration_quality": self._assess_collaboration_quality(results)
        }
    
    def _identify_required_specialists(self, task: str) -> Dict[str, Any]:
        """Identifier les sp√©cialistes n√©cessaires pour une t√¢che"""
        task_lower = task.lower()
        required = {}
        
        # Logique de s√©lection bas√©e sur les capacit√©s
        for agent_id, agent in self.specialist_agents.items():
            relevance_score = 0
            
            # Calculer la pertinence selon les capacit√©s
            for capability in agent.state.capabilities:
                if any(keyword in task_lower for keyword in self._get_capability_keywords(capability)):
                    relevance_score += 1
            
            if relevance_score > 0:
                required[agent_id] = {
                    "agent": agent,
                    "relevance": relevance_score,
                    "role": agent.state.role,
                    "capabilities": agent.state.capabilities
                }
        
        # Trier par pertinence
        required = dict(sorted(required.items(), key=lambda x: x[1]["relevance"], reverse=True))
        
        print(f"üîç Sp√©cialistes identifi√©s: {len(required)}")
        for agent_id, info in required.items():
            print(f"   ‚Ä¢ {agent_id}: {info['role'].value} (pertinence: {info['relevance']})")
        
        return required
    
    def _get_capability_keywords(self, capability: str) -> List[str]:
        """Obtenir les mots-cl√©s associ√©s √† une capacit√©"""
        keyword_map = {
            "research": ["recherche", "information", "√©tude", "analyse", "investigation"],
            "analysis": ["analyse", "√©valuation", "examen", "diagnostic", "assessment"],
            "writing": ["√©criture", "r√©daction", "texte", "contenu", "rapport"],
            "calculation": ["calcul", "math√©matique", "nombre", "statistique", "computation"],
            "validation": ["validation", "v√©rification", "contr√¥le", "test", "quality"],
            "synthesis": ["synth√®se", "r√©sum√©", "compilation", "agr√©gation", "fusion"]
        }
        return keyword_map.get(capability, [capability])
    
    async def _create_collaboration_workflow(self, task: str, specialists: Dict) -> Dict[str, Any]:
        """Cr√©er un workflow de collaboration"""
        workflow = {
            "id": f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "task": task,
            "participants": list(specialists.keys()),
            "steps": [],
            "dependencies": {},
            "estimated_duration": 0
        }
        
        # Cr√©er les √©tapes selon les sp√©cialistes disponibles
        step_id = 1
        
        # √âtape 1: Recherche (si disponible)
        research_agents = [aid for aid, info in specialists.items() 
                          if info["role"] == AgentRole.RESEARCHER]
        if research_agents:
            workflow["steps"].append({
                "id": step_id,
                "type": "research",
                "assigned_to": research_agents[0],
                "description": f"Rechercher des informations sur: {task}",
                "priority": 1
            })
            step_id += 1
        
        # √âtape 2: Analyse (si disponible)
        analysis_agents = [aid for aid, info in specialists.items() 
                          if "analysis" in info["capabilities"]]
        if analysis_agents:
            workflow["steps"].append({
                "id": step_id,
                "type": "analysis",
                "assigned_to": analysis_agents[0],
                "description": f"Analyser les donn√©es pour: {task}",
                "priority": 2,
                "depends_on": [1] if research_agents else []
            })
            step_id += 1
        
        # √âtape 3: Synth√®se finale
        workflow["steps"].append({
            "id": step_id,
            "type": "synthesis",
            "assigned_to": self.state.agent_id,
            "description": "Synth√©tiser tous les r√©sultats",
            "priority": 3,
            "depends_on": list(range(1, step_id))
        })
        
        print(f"üìã Workflow cr√©√©: {len(workflow['steps'])} √©tapes")
        return workflow
    
    async def _execute_workflow(self, workflow: Dict) -> Dict[str, Any]:
        """Ex√©cuter le workflow de collaboration"""
        results = {}
        
        for step in workflow["steps"]:
            print(f"\n‚ö° Ex√©cution √©tape {step['id']}: {step['description']}")
            
            # V√©rifier les d√©pendances
            dependencies_met = all(
                dep_id in results for dep_id in step.get("depends_on", [])
            )
            
            if not dependencies_met:
                print(f"‚è≥ En attente des d√©pendances pour l'√©tape {step['id']}")
                continue
            
            # Ex√©cuter l'√©tape
            if step["assigned_to"] == self.state.agent_id:
                # L'orchestrateur fait la synth√®se
                result = await self._orchestrator_synthesis(workflow, results)
            else:
                # D√©l√©guer √† un sp√©cialiste
                specialist = self.specialist_agents[step["assigned_to"]]
                
                # Pr√©parer le contexte
                context = {
                    "task": workflow["task"],
                    "step_description": step["description"],
                    "previous_results": {k: v for k, v in results.items() if k in step.get("depends_on", [])}
                }
                
                result = await specialist.execute_specialized_task(context)
            
            results[step["id"]] = {
                "step": step,
                "result": result,
                "timestamp": datetime.now().isoformat(),
                "success": result.get("success", True)
            }
            
            print(f"‚úÖ √âtape {step['id']} termin√©e: {result.get('success', True)}")
        
        return results
    
    async def _orchestrator_synthesis(self, workflow: Dict, step_results: Dict) -> Dict[str, Any]:
        """Synth√®se par l'orchestrateur"""
        synthesis_content = f"Synth√®se de la t√¢che: {workflow['task']}\n\n"
        
        for step_id, step_data in step_results.items():
            step_info = step_data["step"]
            result = step_data["result"]
            
            synthesis_content += f"√âtape {step_id} ({step_info['type']}):\n"
            synthesis_content += f"- Description: {step_info['description']}\n"
            synthesis_content += f"- R√©sultat: {result.get('content', 'Pas de contenu')}\n"
            synthesis_content += f"- Succ√®s: {result.get('success', 'Unknown')}\n\n"
        
        return {
            "content": synthesis_content,
            "synthesis_quality": len(step_results) / len(workflow["steps"]),
            "success": all(r["success"] for r in step_results.values()),
            "type": "orchestrator_synthesis"
        }
    
    async def _synthesize_results(self, task: str, results: Dict) -> Dict[str, Any]:
        """Synth√©tiser les r√©sultats finaux"""
        successful_steps = sum(1 for r in results.values() if r["success"])
        total_steps = len(results)
        
        # Compiler les contenus
        all_content = []
        for step_id, step_data in results.items():
            content = step_data["result"].get("content", "")
            if content:
                all_content.append(f"√âtape {step_id}: {content}")
        
        synthesis = {
            "task_completed": task,
            "total_steps": total_steps,
            "successful_steps": successful_steps,
            "success_rate": successful_steps / total_steps if total_steps > 0 else 0,
            "final_synthesis": "\n".join(all_content),
            "success": successful_steps == total_steps,
            "quality_score": successful_steps / total_steps if total_steps > 0 else 0
        }
        
        return synthesis
    
    def _assess_collaboration_quality(self, results: Dict) -> float:
        """√âvaluer la qualit√© de la collaboration"""
        if not results:
            return 0.0
        
        # Facteurs de qualit√©
        completion_rate = sum(1 for r in results.values() if r["success"]) / len(results)
        
        # Communication quality (bas√© sur les messages √©chang√©s)
        communication_quality = min(1.0, len(self.state.collaboration_history) / len(results))
        
        # Temps de traitement (simulation)
        timing_quality = 0.8  # Simul√©
        
        overall_quality = (completion_rate * 0.5 + communication_quality * 0.3 + timing_quality * 0.2)
        
        return overall_quality

class ResearchSpecialist(AdvancedAgentBase):
    """Agent sp√©cialis√© en recherche d'informations"""
    
    def __init__(self):
        super().__init__(
            agent_id="researcher_001",
            role=AgentRole.RESEARCHER,
            capabilities=["research", "information_gathering", "fact_checking"]
        )
    
    async def execute_specialized_task(self, context: Dict) -> Dict[str, Any]:
        """Ex√©cuter une t√¢che de recherche sp√©cialis√©e"""
        task = context.get("step_description", "")
        main_task = context.get("task", "")
        
        print(f"üîç RECHERCHE SP√âCIALIS√âE: {task}")
        
        # Simuler une recherche approfondie
        research_results = {
            "search_queries": self._generate_search_queries(main_task),
            "information_found": self._simulate_research(main_task),
            "sources": self._generate_sources(main_task),
            "confidence_level": 0.85
        }
        
        # Compiler le rapport de recherche
        content = self._compile_research_report(research_results)
        
        return {
            "content": content,
            "research_data": research_results,
            "success": True,
            "confidence": research_results["confidence_level"],
            "specialist": "researcher"
        }
    
    def _generate_search_queries(self, task: str) -> List[str]:
        """G√©n√©rer des requ√™tes de recherche"""
        base_terms = task.split()
        queries = [
            f"what is {task}",
            f"{task} definition explanation",
            f"{task} examples applications",
            f"latest research {task}",
            f"best practices {task}"
        ]
        return queries[:3]  # Limiter pour la d√©mo
    
    def _simulate_research(self, task: str) -> Dict[str, Any]:
        """Simuler des r√©sultats de recherche"""
        return {
            "key_concepts": [f"Concept A de {task}", f"Concept B de {task}", f"Concept C de {task}"],
            "main_benefits": [f"B√©n√©fice 1", f"B√©n√©fice 2", f"B√©n√©fice 3"],
            "challenges": [f"D√©fi 1", f"D√©fi 2"],
            "current_trends": [f"Tendance 1 en {task}", f"Tendance 2 en {task}"]
        }
    
    def _generate_sources(self, task: str) -> List[Dict]:
        """G√©n√©rer des sources simul√©es"""
        return [
            {"title": f"Guide complet sur {task}", "url": f"https://expert-guide.com/{task.replace(' ', '-')}", "reliability": 0.9},
            {"title": f"Recherche r√©cente: {task}", "url": f"https://research.org/papers/{task.replace(' ', '_')}", "reliability": 0.95},
            {"title": f"Best practices pour {task}", "url": f"https://bestpractices.com/{task.replace(' ', '-')}", "reliability": 0.8}
        ]
    
    def _compile_research_report(self, research_data: Dict) -> str:
        """Compiler un rapport de recherche"""
        report = f"""RAPPORT DE RECHERCHE

Concepts cl√©s identifi√©s:
"""
        for concept in research_data["information_found"]["key_concepts"]:
            report += f"‚Ä¢ {concept}\n"
        
        report += f"\nB√©n√©fices principaux:\n"
        for benefit in research_data["information_found"]["main_benefits"]:
            report += f"‚Ä¢ {benefit}\n"
        
        report += f"\nD√©fis identifi√©s:\n"
        for challenge in research_data["information_found"]["challenges"]:
            report += f"‚Ä¢ {challenge}\n"
        
        report += f"\nTendances actuelles:\n"
        for trend in research_data["information_found"]["current_trends"]:
            report += f"‚Ä¢ {trend}\n"
        
        report += f"\nSources consult√©es: {len(research_data['sources'])} sources fiables"
        report += f"\nNiveau de confiance: {research_data['confidence_level']:.1%}"
        
        return report

class AnalysisSpecialist(AdvancedAgentBase):
    """Agent sp√©cialis√© en analyse de donn√©es"""
    
    def __init__(self):
        super().__init__(
            agent_id="analyst_001",
            role=AgentRole.SPECIALIST,
            capabilities=["analysis", "data_processing", "insights_generation"]
        )
    
    async def execute_specialized_task(self, context: Dict) -> Dict[str, Any]:
        """Ex√©cuter une t√¢che d'analyse sp√©cialis√©e"""
        task = context.get("step_description", "")
        previous_results = context.get("previous_results", {})
        
        print(f"üìä ANALYSE SP√âCIALIS√âE: {task}")
        
        # Analyser les donn√©es des √©tapes pr√©c√©dentes
        analysis_results = self._perform_analysis(previous_results, task)
        
        # G√©n√©rer des insights
        insights = self._generate_insights(analysis_results)
        
        # Compiler le rapport d'analyse
        content = self._compile_analysis_report(analysis_results, insights)
        
        return {
            "content": content,
            "analysis_data": analysis_results,
            "insights": insights,
            "success": True,
            "confidence": 0.88,
            "specialist": "analyst"
        }
    
    def _perform_analysis(self, previous_results: Dict, task: str) -> Dict[str, Any]:
        """Effectuer l'analyse des donn√©es pr√©c√©dentes"""
        analysis = {
            "data_quality": self._assess_data_quality(previous_results),
            "key_patterns": self._identify_patterns(previous_results),
            "statistical_summary": self._generate_statistics(previous_results),
            "correlations": self._find_correlations(previous_results)
        }
        
        return analysis
    
    def _assess_data_quality(self, data: Dict) -> Dict[str, Any]:
        """√âvaluer la qualit√© des donn√©es"""
        if not data:
            return {"score": 0.0, "issues": ["Aucune donn√©e disponible"]}
        
        completeness = len([r for r in data.values() if r.get("success", False)]) / len(data)
        
        return {
            "score": completeness,
            "completeness": completeness,
            "issues": [] if completeness > 0.8 else ["Donn√©es incompl√®tes"],
            "recommendations": ["Donn√©es de qualit√© acceptable"] if completeness > 0.8 else ["Am√©liorer la collecte de donn√©es"]
        }
    
    def _identify_patterns(self, data: Dict) -> List[str]:
        """Identifier des patterns dans les donn√©es"""
        patterns = []
        
        if data:
            successful_steps = sum(1 for r in data.values() if r.get("success", False))
            total_steps = len(data)
            
            if successful_steps == total_steps:
                patterns.append("Workflow ex√©cut√© avec succ√®s complet")
            elif successful_steps > total_steps * 0.5:
                patterns.append("Majorit√© des √©tapes r√©ussies")
            else:
                patterns.append("Performance workflow √† am√©liorer")
        
        return patterns
    
    def _generate_statistics(self, data: Dict) -> Dict[str, Any]:
        """G√©n√©rer des statistiques"""
        if not data:
            return {"total_steps": 0, "success_rate": 0.0}
        
        total_steps = len(data)
        successful_steps = sum(1 for r in data.values() if r.get("success", False))
        
        return {
            "total_steps": total_steps,
            "successful_steps": successful_steps,
            "success_rate": successful_steps / total_steps if total_steps > 0 else 0,
            "failure_rate": 1 - (successful_steps / total_steps) if total_steps > 0 else 0
        }
    
    def _find_correlations(self, data: Dict) -> List[str]:
        """Trouver des corr√©lations"""
        correlations = []
        
        if len(data) >= 2:
            correlations.append("Corr√©lation positive entre qualit√© des donn√©es et succ√®s")
            correlations.append("Les √©tapes s√©quentielles montrent une d√©pendance logique")
        
        return correlations
    
    def _generate_insights(self, analysis: Dict) -> List[str]:
        """G√©n√©rer des insights bas√©s sur l'analyse"""
        insights = []
        
        quality_score = analysis["data_quality"]["score"]
        if quality_score > 0.8:
            insights.append("Excellent niveau de qualit√© des donn√©es collect√©es")
        elif quality_score > 0.6:
            insights.append("Qualit√© des donn√©es acceptable avec marge d'am√©lioration")
        else:
            insights.append("Qualit√© des donn√©es insuffisante, r√©vision n√©cessaire")
        
        success_rate = analysis["statistical_summary"]["success_rate"]
        if success_rate > 0.9:
            insights.append("Performance exceptionnelle du workflow")
        elif success_rate > 0.7:
            insights.append("Bonne performance globale")
        else:
            insights.append("Performance √† optimiser")
        
        return insights
    
    def _compile_analysis_report(self, analysis: Dict, insights: List[str]) -> str:
        """Compiler le rapport d'analyse"""
        report = f"""RAPPORT D'ANALYSE

Qualit√© des donn√©es:
‚Ä¢ Score global: {analysis['data_quality']['score']:.1%}
‚Ä¢ Compl√©tude: {analysis['data_quality']['completeness']:.1%}

Statistiques cl√©s:
‚Ä¢ Total d'√©tapes: {analysis['statistical_summary']['total_steps']}
‚Ä¢ Taux de succ√®s: {analysis['statistical_summary']['success_rate']:.1%}

Patterns identifi√©s:
"""
        for pattern in analysis["key_patterns"]:
            report += f"‚Ä¢ {pattern}\n"
        
        report += f"\nInsights principaux:\n"
        for insight in insights:
            report += f"‚Ä¢ {insight}\n"
        
        report += f"\nCorr√©lations d√©tect√©es:\n"
        for correlation in analysis["correlations"]:
            report += f"‚Ä¢ {correlation}\n"
        
        return report

# Fonction de d√©monstration
async def demo_multi_agent_collaboration():
    """D√©monstration du pattern Multi-Agent Collaboration"""
    print(f"\nüé¨ D√âMONSTRATION: Pattern Multi-Agent Collaboration")
    print("=" * 80)
    
    # Cr√©er l'orchestrateur
    orchestrator = OrchestratorAgent()
    
    # Cr√©er les sp√©cialistes
    researcher = ResearchSpecialist()
    analyst = AnalysisSpecialist()
    
    # Enregistrer les sp√©cialistes
    orchestrator.register_specialist(researcher)
    orchestrator.register_specialist(analyst)
    
    # T√¢ches de test pour collaboration
    test_tasks = [
        "Recherche et analyse des tendances en intelligence artificielle",
        "√âtude de march√© pour les agents conversationnels",
        "Analyse comparative des frameworks d'IA"
    ]
    
    results = []
    
    for i, task in enumerate(test_tasks, 1):
        print(f"\n--- T√¢che collaborative {i}/{len(test_tasks)} ---")
        
        result = await orchestrator.coordinate_task(task)
        results.append(result)
        
        print(f"\nüìä R√©sultats t√¢che {i}:")
        print(f"   ‚Ä¢ Sp√©cialistes impliqu√©s: {len(result['specialists_involved'])}")
        print(f"   ‚Ä¢ √âtapes du workflow: {result['workflow_steps']}")
        print(f"   ‚Ä¢ Succ√®s global: {'Oui' if result['success'] else 'Non'}")
        print(f"   ‚Ä¢ Qualit√© collaboration: {result['collaboration_quality']:.2f}")
        print(f"   ‚Ä¢ Synth√®se: {result['synthesized_result']['final_synthesis'][:100]}...")
    
    # Statistiques finales
    print(f"\nüìà STATISTIQUES COLLABORATION")
    print("=" * 60)
    total_tasks = len(results)
    successful_tasks = sum(1 for r in results if r["success"])
    avg_quality = sum(r["collaboration_quality"] for r in results) / total_tasks
    
    print(f"üìä Total t√¢ches: {total_tasks}")
    print(f"‚úÖ T√¢ches r√©ussies: {successful_tasks} ({successful_tasks/total_tasks:.1%})")
    print(f"üèÜ Qualit√© moyenne: {avg_quality:.2f}")
    
    return results
```

### ‚úÖ TODO 3: Pattern 2 - Self-Correction (15 min)

**Concepts appris**: Auto-am√©lioration et boucles de feedback

Impl√©mentez le syst√®me d'auto-correction:

```python
class SelfCorrectionAgent(AdvancedAgentBase):
    """Agent avec capacit√©s d'auto-correction et d'am√©lioration continue"""
    
    def __init__(self):
        super().__init__(
            agent_id="self_corrector_001",
            role=AgentRole.VALIDATOR,
            capabilities=["self_assessment", "error_detection", "improvement", "learning"]
        )
        
        self.correction_history = []
        self.quality_thresholds = {
            "accuracy": 0.8,
            "completeness": 0.7,
            "relevance": 0.75,
            "coherence": 0.8
        }
        self.max_correction_iterations = 3
        
    async def process_with_self_correction(self, task: str, initial_response: str = None) -> Dict[str, Any]:
        """Traiter une t√¢che avec auto-correction"""
        print(f"\nüîÑ SELF-CORRECTION: {task}")
        print("=" * 60)
        
        # G√©n√©rer une r√©ponse initiale si non fournie
        if initial_response is None:
            initial_response = await self._generate_initial_response(task)
        
        current_response = initial_response
        iteration = 0
        correction_log = []
        
        while iteration < self.max_correction_iterations:
            iteration += 1
            print(f"\nüîç It√©ration {iteration}: √âvaluation et correction")
            
            # √âvaluer la qualit√© de la r√©ponse actuelle
            quality_assessment = await self._assess_quality(task, current_response)
            
            print(f"üìä Scores de qualit√©:")
            for metric, score in quality_assessment["scores"].items():
                status = "‚úÖ" if score >= self.quality_thresholds[metric] else "‚ùå"
                print(f"   ‚Ä¢ {metric}: {score:.2f} {status}")
            
            # V√©rifier si des corrections sont n√©cessaires
            needs_correction = quality_assessment["needs_correction"]
            
            if not needs_correction:
                print(f"‚úÖ Qualit√© satisfaisante atteinte √† l'it√©ration {iteration}")
                break
            
            # Identifier les probl√®mes sp√©cifiques
            issues = quality_assessment["identified_issues"]
            print(f"üö® Probl√®mes identifi√©s: {len(issues)}")
            for issue in issues:
                print(f"   ‚Ä¢ {issue}")
            
            # G√©n√©rer une version corrig√©e
            correction_strategy = self._develop_correction_strategy(quality_assessment)
            corrected_response = await self._apply_corrections(
                current_response, 
                correction_strategy, 
                task
            )
            
            # Enregistrer le cycle de correction
            correction_cycle = {
                "iteration": iteration,
                "original_response": current_response,
                "quality_scores": quality_assessment["scores"],
                "issues_found": issues,
                "correction_strategy": correction_strategy,
                "corrected_response": corrected_response,
                "timestamp": datetime.now().isoformat()
            }
            
            correction_log.append(correction_cycle)
            current_response = corrected_response
            
            print(f"üîß Correction appliqu√©e pour l'it√©ration {iteration}")
        
        # √âvaluation finale
        final_assessment = await self._assess_quality(task, current_response)
        
        # Apprendre de cette exp√©rience
        learning_insights = self._extract_learning_insights(correction_log, final_assessment)
        for insight in learning_insights:
            self.add_learning_insight(insight)
        
        result = {
            "task": task,
            "initial_response": initial_response,
            "final_response": current_response,
            "correction_iterations": iteration,
            "correction_log": correction_log,
            "final_quality_scores": final_assessment["scores"],
            "improvement_achieved": self._calculate_improvement(correction_log, final_assessment),
            "learning_insights": learning_insights,
            "success": not final_assessment["needs_correction"],
            "quality_threshold_met": all(
                score >= threshold 
                for metric, (score, threshold) in zip(
                    final_assessment["scores"].keys(),
                    zip(final_assessment["scores"].values(), self.quality_thresholds.values())
                )
            )
        }
        
        self.correction_history.append(result)
        
        print(f"\nüèÜ R√âSULTAT FINAL:")
        print(f"   ‚Ä¢ It√©rations: {iteration}")
        print(f"   ‚Ä¢ Qualit√© finale: {sum(final_assessment['scores'].values())/len(final_assessment['scores']):.2f}")
        print(f"   ‚Ä¢ Seuils atteints: {'Oui' if result['quality_threshold_met'] else 'Non'}")
        
        return result
    
    async def _generate_initial_response(self, task: str) -> str:
        """G√©n√©rer une r√©ponse initiale"""
        # Simulation d'une r√©ponse LLM initiale
        initial_response = f"""R√©ponse initiale pour: {task}

Voici une premi√®re approche de r√©ponse qui pourrait contenir des impr√©cisions ou √™tre incompl√®te. Cette r√©ponse sera ensuite √©valu√©e et am√©lior√©e par le syst√®me d'auto-correction.

Contenu principal: {task} est un sujet important qui n√©cessite une analyse approfondie. Les aspects cl√©s incluent plusieurs √©l√©ments qu'il faut consid√©rer attentivement.

Points √† d√©velopper:
- Aspect technique
- Consid√©rations pratiques  
- Implications futures

Cette r√©ponse initiale sera affin√©e lors des it√©rations suivantes."""
        
        return initial_response
    
    async def _assess_quality(self, task: str, response: str) -> Dict[str, Any]:
        """√âvaluer la qualit√© d'une r√©ponse"""
        
        # √âvaluation de l'exactitude
        accuracy_score = self._assess_accuracy(task, response)
        
        # √âvaluation de la compl√©tude
        completeness_score = self._assess_completeness(task, response)
        
        # √âvaluation de la pertinence
        relevance_score = self._assess_relevance(task, response)
        
        # √âvaluation de la coh√©rence
        coherence_score = self._assess_coherence(response)
        
        scores = {
            "accuracy": accuracy_score,
            "completeness": completeness_score,
            "relevance": relevance_score,
            "coherence": coherence_score
        }
        
        # Identifier les probl√®mes
        issues = []
        for metric, score in scores.items():
            if score < self.quality_thresholds[metric]:
                issues.append(f"{metric} insuffisant ({score:.2f} < {self.quality_thresholds[metric]})")
        
        # D√©terminer si une correction est n√©cessaire
        needs_correction = len(issues) > 0
        
        assessment = {
            "scores": scores,
            "average_score": sum(scores.values()) / len(scores),
            "identified_issues": issues,
            "needs_correction": needs_correction,
            "quality_level": self._determine_quality_level(scores)
        }
        
        return assessment
    
    def _assess_accuracy(self, task: str, response: str) -> float:
        """√âvaluer l'exactitude de la r√©ponse"""
        # Simulation d'√©valuation d'exactitude
        response_lower = response.lower()
        task_lower = task.lower()
        
        # V√©rifier la correspondance avec la t√¢che
        task_relevance = 0.7  # Score de base
        
        # Bonus pour des termes techniques appropri√©s
        if "analyse" in response_lower and "analyse" in task_lower:
            task_relevance += 0.1
        if "important" in response_lower:
            task_relevance += 0.05
        
        # P√©nalit√© pour des r√©ponses trop vagues
        if len(response.split()) < 50:
            task_relevance -= 0.2
        
        return max(0.0, min(1.0, task_relevance))
    
    def _assess_completeness(self, task: str, response: str) -> float:
        """√âvaluer la compl√©tude de la r√©ponse"""
        expected_elements = [
            "introduction", "d√©veloppement", "conclusion",
            "exemples", "aspects", "points"
        ]
        
        response_lower = response.lower()
        elements_present = sum(1 for element in expected_elements if element in response_lower)
        
        # Score bas√© sur la pr√©sence d'√©l√©ments attendus
        completeness = elements_present / len(expected_elements)
        
        # Bonus pour la longueur appropri√©e
        word_count = len(response.split())
        if word_count >= 100:
            completeness += 0.1
        elif word_count < 30:
            completeness -= 0.2
        
        return max(0.0, min(1.0, completeness))
    
    def _assess_relevance(self, task: str, response: str) -> float:
        """√âvaluer la pertinence de la r√©ponse"""
        task_words = set(task.lower().split())
        response_words = set(response.lower().split())
        
        # Intersection des mots-cl√©s
        common_words = task_words.intersection(response_words)
        
        if len(task_words) == 0:
            return 0.5  # Score neutre si pas de mots dans la t√¢che
        
        keyword_overlap = len(common_words) / len(task_words)
        
        # Bonus pour la mention explicite de la t√¢che
        if task.lower() in response.lower():
            keyword_overlap += 0.2
        
        return max(0.0, min(1.0, keyword_overlap))
    
    def _assess_coherence(self, response: str) -> float:
        """√âvaluer la coh√©rence de la r√©ponse"""
        sentences = response.split('.')
        
        # Score de base pour les r√©ponses structur√©es
        coherence = 0.6
        
        # Bonus pour la structure
        if len(sentences) >= 3:
            coherence += 0.1
        
        # Bonus pour la pr√©sence de connecteurs logiques
        connectors = ["donc", "ainsi", "par cons√©quent", "cependant", "en effet", "de plus"]
        connector_count = sum(1 for conn in connectors if conn in response.lower())
        coherence += min(0.3, connector_count * 0.1)
        
        # P√©nalit√© pour r√©p√©titions excessives
        words = response.lower().split()
        unique_words = set(words)
        if len(words) > 0:
            repetition_ratio = len(unique_words) / len(words)
            if repetition_ratio < 0.5:
                coherence -= 0.2
        
        return max(0.0, min(1.0, coherence))
    
    def _determine_quality_level(self, scores: Dict[str, float]) -> str:
        """D√©terminer le niveau de qualit√© global"""
        avg_score = sum(scores.values()) / len(scores)
        
        if avg_score >= 0.9:
            return "Excellent"
        elif avg_score >= 0.8:
            return "Tr√®s bon"
        elif avg_score >= 0.7:
            return "Bon"
        elif avg_score >= 0.6:
            return "Acceptable"
        else:
            return "Insuffisant"
    
    def _develop_correction_strategy(self, quality_assessment: Dict) -> Dict[str, Any]:
        """D√©velopper une strat√©gie de correction"""
        scores = quality_assessment["scores"]
        issues = quality_assessment["identified_issues"]
        
        strategy = {
            "corrections_needed": [],
            "priority_order": [],
            "specific_actions": []
        }
        
        # Identifier les corrections par priorit√©
        for metric, score in scores.items():
            if score < self.quality_thresholds[metric]:
                deficit = self.quality_thresholds[metric] - score
                strategy["corrections_needed"].append({
                    "metric": metric,
                    "current_score": score,
                    "target_score": self.quality_thresholds[metric],
                    "deficit": deficit,
                    "priority": deficit  # Plus le d√©ficit est grand, plus la priorit√© est haute
                })
        
        # Trier par priorit√©
        strategy["corrections_needed"].sort(key=lambda x: x["deficit"], reverse=True)
        strategy["priority_order"] = [c["metric"] for c in strategy["corrections_needed"]]
        
        # Actions sp√©cifiques selon le type de probl√®me
        for correction in strategy["corrections_needed"]:
            metric = correction["metric"]
            
            if metric == "accuracy":
                strategy["specific_actions"].append("V√©rifier et corriger les faits inexacts")
                strategy["specific_actions"].append("Ajouter des d√©tails pr√©cis")
                
            elif metric == "completeness":
                strategy["specific_actions"].append("D√©velopper les sections manquantes")
                strategy["specific_actions"].append("Ajouter des exemples concrets")
                
            elif metric == "relevance":
                strategy["specific_actions"].append("Recentrer sur le sujet principal")
                strategy["specific_actions"].append("Supprimer les √©l√©ments hors-sujet")
                
            elif metric == "coherence":
                strategy["specific_actions"].append("Am√©liorer les transitions entre id√©es")
                strategy["specific_actions"].append("Restructurer le contenu logiquement")
        
        return strategy
    
    async def _apply_corrections(self, original_response: str, strategy: Dict, task: str) -> str:
        """Appliquer les corrections selon la strat√©gie"""
        corrected_response = original_response
        
        # Appliquer les corrections selon la priorit√©
        for action in strategy["specific_actions"]:
            if "V√©rifier et corriger les faits" in action:
                corrected_response = self._improve_accuracy(corrected_response, task)
            elif "D√©velopper les sections" in action:
                corrected_response = self._improve_completeness(corrected_response, task)
            elif "Recentrer sur le sujet" in action:
                corrected_response = self._improve_relevance(corrected_response, task)
            elif "Am√©liorer les transitions" in action:
                corrected_response = self._improve_coherence(corrected_response)
        
        return corrected_response
    
    def _improve_accuracy(self, response: str, task: str) -> str:
        """Am√©liorer l'exactitude de la r√©ponse"""
        # Ajout de pr√©cisions et corrections factuelles
        improved = response + f"\n\n[Correction d'exactitude] Pr√©cisions importantes concernant {task}:\n"
        improved += "- Informations v√©rifi√©es et valid√©es\n"
        improved += "- Sources fiables consult√©es\n"
        improved += "- Donn√©es mises √† jour selon les derni√®res recherches\n"
        
        return improved
    
    def _improve_completeness(self, response: str, task: str) -> str:
        """Am√©liorer la compl√©tude de la r√©ponse"""
        # Ajout de sections manquantes
        improved = response + f"\n\n[Am√©lioration compl√©tude] D√©veloppements suppl√©mentaires:\n"
        improved += f"‚Ä¢ Contexte d√©taill√©: {task} s'inscrit dans un cadre plus large qu'il convient d'analyser.\n"
        improved += "‚Ä¢ Exemples pratiques: Plusieurs cas d'usage illustrent concr√®tement les concepts.\n"
        improved += "‚Ä¢ Implications futures: Les tendances √©mergentes sugg√®rent des √©volutions importantes.\n"
        improved += "‚Ä¢ Consid√©rations techniques: Les aspects m√©thodologiques m√©ritent une attention particuli√®re.\n"
        
        return improved
    
    def _improve_relevance(self, response: str, task: str) -> str:
        """Am√©liorer la pertinence de la r√©ponse"""
        # Recentrage sur le sujet principal
        improved = f"[Recentrage sur {task}]\n\n" + response
        improved += f"\n\n[Synth√®se pertinence] En r√©sum√©, concernant sp√©cifiquement {task}:\n"
        improved += "- Les √©l√©ments cl√©s ont √©t√© identifi√©s et analys√©s\n"
        improved += "- La r√©ponse se concentre sur les aspects les plus pertinents\n"
        improved += "- Les informations p√©riph√©riques ont √©t√© √©cart√©es pour plus de clart√©\n"
        
        return improved
    
    def _improve_coherence(self, response: str) -> str:
        """Am√©liorer la coh√©rence de la r√©ponse"""
        # Am√©lioration de la structure et des transitions
        sections = response.split('\n\n')
        
        improved_sections = []
        for i, section in enumerate(sections):
            if i > 0:
                # Ajouter des connecteurs logiques
                connectors = ["Par ailleurs,", "En outre,", "De plus,", "Cependant,", "Ainsi,"]
                if not any(conn.lower() in section.lower() for conn in connectors):
                    connector = connectors[i % len(connectors)]
                    section = f"{connector} {section}"
            
            improved_sections.append(section)
        
        improved = '\n\n'.join(improved_sections)
        improved += "\n\n[Am√©lioration coh√©rence] La structure logique a √©t√© renforc√©e avec des transitions appropri√©es entre les diff√©rentes sections."
        
        return improved
    
    def _calculate_improvement(self, correction_log: List[Dict], final_assessment: Dict) -> Dict[str, Any]:
        """Calculer l'am√©lioration r√©alis√©e"""
        if not correction_log:
            return {"improvement": 0, "details": "Aucune correction effectu√©e"}
        
        initial_scores = correction_log[0]["quality_scores"]
        final_scores = final_assessment["scores"]
        
        improvements = {}
        total_improvement = 0
        
        for metric in initial_scores.keys():
            initial = initial_scores[metric]
            final = final_scores[metric]
            improvement = final - initial
            improvements[metric] = {
                "initial": initial,
                "final": final,
                "improvement": improvement,
                "improvement_percentage": (improvement / initial * 100) if initial > 0 else 0
            }
            total_improvement += improvement
        
        return {
            "total_improvement": total_improvement,
            "average_improvement": total_improvement / len(initial_scores),
            "by_metric": improvements,
            "significant_improvement": total_improvement > 0.2,
            "best_improved_metric": max(improvements.keys(), key=lambda x: improvements[x]["improvement"])
        }
    
    def _extract_learning_insights(self, correction_log: List[Dict], final_assessment: Dict) -> List[str]:
        """Extraire des insights d'apprentissage"""
        insights = []
        
        if correction_log:
            iterations_needed = len(correction_log)
            
            if iterations_needed == 1:
                insights.append("Corrections mineures n√©cessaires - bon niveau initial")
            elif iterations_needed == 2:
                insights.append("Corrections mod√©r√©es - processus d'am√©lioration efficace")
            else:
                insights.append("Corrections importantes n√©cessaires - r√©vision du processus initial recommand√©e")
            
            # Analyser les patterns de correction
            common_issues = []
            for log_entry in correction_log:
                for issue in log_entry["issues_found"]:
                    if "accuracy" in issue:
                        common_issues.append("accuracy")
                    elif "completeness" in issue:
                        common_issues.append("completeness")
                    elif "relevance" in issue:
                        common_issues.append("relevance")
                    elif "coherence" in issue:
                        common_issues.append("coherence")
            
            from collections import Counter
            issue_counts = Counter(common_issues)
            
            if issue_counts:
                most_common_issue = issue_counts.most_common(1)[0][0]
                insights.append(f"Probl√®me r√©current identifi√©: {most_common_issue}")
        
        # Analyser la qualit√© finale
        final_avg_score = sum(final_assessment["scores"].values()) / len(final_assessment["scores"])
        if final_avg_score >= 0.9:
            insights.append("Excellente qualit√© finale atteinte")
        elif final_avg_score >= 0.8:
            insights.append("Bonne qualit√© finale avec marge d'am√©lioration")
        else:
            insights.append("Qualit√© finale insuffisante - r√©vision du processus n√©cessaire")
        
        return insights
    
    async def demonstrate_self_correction(self):
        """D√©monstration des capacit√©s d'auto-correction"""
        print(f"\nüé¨ D√âMONSTRATION: Pattern Self-Correction")
        print("=" * 80)
        
        demo_tasks = [
            "Expliquer les principes de l'intelligence artificielle",
            "Analyser les avantages et inconv√©nients du t√©l√©travail",
            "Pr√©senter les enjeux de la cybers√©curit√© moderne"
        ]
        
        results = []
        
        for i, task in enumerate(demo_tasks, 1):
            print(f"\n--- T√¢che d'auto-correction {i}/{len(demo_tasks)} ---")
            print(f"üìù T√¢che: {task}")
            
            result = await self.process_with_self_correction(task)
            results.append(result)
            
            print(f"\nüìä R√©sultats pour la t√¢che {i}:")
            print(f"   ‚Ä¢ It√©rations de correction: {result['correction_iterations']}")
            print(f"   ‚Ä¢ Seuils de qualit√© atteints: {'Oui' if result['quality_threshold_met'] else 'Non'}")
            print(f"   ‚Ä¢ Am√©lioration totale: {result['improvement_achieved']['total_improvement']:.2f}")
            print(f"   ‚Ä¢ M√©trique la plus am√©lior√©e: {result['improvement_achieved'].get('best_improved_metric', 'N/A')}")
            print(f"   ‚Ä¢ Insights d'apprentissage: {len(result['learning_insights'])}")
        
        # Statistiques globales
        print(f"\nüìà STATISTIQUES SELF-CORRECTION")
        print("=" * 60)
        
        total_tasks = len(results)
        successful_corrections = sum(1 for r in results if r["quality_threshold_met"])
        avg_iterations = sum(r["correction_iterations"] for r in results) / total_tasks
        total_insights = sum(len(r["learning_insights"]) for r in results)
        
        print(f"üìä Total t√¢ches: {total_tasks}")
        print(f"‚úÖ Corrections r√©ussies: {successful_corrections} ({successful_corrections/total_tasks:.1%})")
        print(f"üîÑ It√©rations moyennes: {avg_iterations:.1f}")
        print(f"üß† Insights d'apprentissage: {total_insights}")
        print(f"üìö Historique total: {len(self.correction_history)} exp√©riences")
        
        return results
```

### ‚úÖ TODO 4: Pattern 3 - RAG Agent (20 min)

**Concepts appris**: Retrieval-Augmented Generation et base de connaissances

Impl√©mentez le syst√®me RAG avanc√©:

```python
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict, Any, Optional

class AdvancedRAGAgent(AdvancedAgentBase):
    """Agent RAG avanc√© avec base de connaissances vectorielle"""
    
    def __init__(self):
        super().__init__(
            agent_id="rag_agent_001",
            role=AgentRole.RESEARCHER,
            capabilities=["knowledge_retrieval", "semantic_search", "context_generation", "fact_verification"]
        )
        
        # Configuration RAG
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.chroma_client = chromadb.Client()
        self.knowledge_collections = {}
        self.retrieval_config = {
            "top_k": 5,
            "similarity_threshold": 0.6,
            "max_context_length": 2000,
            "rerank_results": True
        }
        
        # Initialiser les collections de connaissances
        self._initialize_knowledge_base()
        
    def _initialize_knowledge_base(self):
        """Initialiser la base de connaissances vectorielle"""
        print("üß† Initialisation de la base de connaissances RAG...")
        
        # Cr√©er les collections th√©matiques
        collection_configs = {
            "ai_knowledge": {
                "description": "Connaissances sur l'intelligence artificielle",
                "metadata": {"domain": "ai", "language": "fr"}
            },
            "technical_docs": {
                "description": "Documentation technique et programmation",
                "metadata": {"domain": "tech", "language": "fr"}
            },
            "business_context": {
                "description": "Contexte business et strat√©gique",
                "metadata": {"domain": "business", "language": "fr"}
            }
        }
        
        for collection_name, config in collection_configs.items():
            try:
                collection = self.chroma_client.create_collection(
                    name=collection_name,
                    metadata=config["metadata"]
                )
                self.knowledge_collections[collection_name] = collection
                print(f"‚úÖ Collection cr√©√©e: {collection_name}")
            except Exception as e:
                # Collection existe d√©j√†
                collection = self.chroma_client.get_collection(collection_name)
                self.knowledge_collections[collection_name] = collection
                print(f"‚ôªÔ∏è Collection existante: {collection_name}")
        
        # Peupler avec des connaissances de base
        self._seed_knowledge_base()
        
        print(f"üéØ Base de connaissances initialis√©e: {len(self.knowledge_collections)} collections")
    
    def _seed_knowledge_base(self):
        """Peupler la base avec des connaissances initiales"""
        
        # Connaissances IA
        ai_documents = [
            {
                "id": "ai_001",
                "content": "L'intelligence artificielle (IA) est une technologie qui permet aux machines de simuler l'intelligence humaine. Elle comprend l'apprentissage automatique, le traitement du langage naturel, la vision par ordinateur et la robotique.",
                "metadata": {"category": "definition", "importance": "high"}
            },
            {
                "id": "ai_002", 
                "content": "Les agents IA sont des syst√®mes autonomes capables de percevoir leur environnement, de prendre des d√©cisions et d'agir pour atteindre des objectifs sp√©cifiques. Ils suivent g√©n√©ralement une boucle Perception ‚Üí Plan ‚Üí Act ‚Üí Reflect.",
                "metadata": {"category": "agents", "importance": "high"}
            },
            {
                "id": "ai_003",
                "content": "Le RAG (Retrieval-Augmented Generation) combine la r√©cup√©ration d'informations avec la g√©n√©ration de texte. Il permet aux mod√®les de langage d'acc√©der √† des connaissances externes sp√©cifiques et actualis√©es.",
                "metadata": {"category": "rag", "importance": "medium"}
            },
            {
                "id": "ai_004",
                "content": "Les LLM (Large Language Models) comme GPT-4 sont des mod√®les de langage pr√©-entra√Æn√©s sur de vastes corpus de texte. Ils excellent dans la compr√©hension et la g√©n√©ration de langage naturel.",
                "metadata": {"category": "llm", "importance": "high"}
            }
        ]
        
        # Documents techniques
        tech_documents = [
            {
                "id": "tech_001",
                "content": "Python est un langage de programmation polyvalent largement utilis√© en IA. Il offre de nombreuses biblioth√®ques comme TensorFlow, PyTorch, scikit-learn pour le machine learning et OpenAI pour les LLM.",
                "metadata": {"category": "programming", "importance": "medium"}
            },
            {
                "id": "tech_002",
                "content": "Les bases de donn√©es vectorielles comme ChromaDB, Pinecone ou Weaviate sont essentielles pour impl√©menter des syst√®mes RAG efficaces. Elles permettent la recherche s√©mantique bas√©e sur la similarit√©.",
                "metadata": {"category": "databases", "importance": "medium"}
            },
            {
                "id": "tech_003",
                "content": "Les embeddings sont des repr√©sentations vectorielles de texte qui capturent le sens s√©mantique. Des mod√®les comme sentence-transformers g√©n√®rent des embeddings de haute qualit√© pour la recherche s√©mantique.",
                "metadata": {"category": "embeddings", "importance": "high"}
            }
        ]
        
        # Contexte business
        business_documents = [
            {
                "id": "biz_001",
                "content": "L'impl√©mentation d'agents IA en entreprise n√©cessite une strat√©gie claire, une gouvernance des donn√©es et une formation des √©quipes. Les ROI peuvent √™tre significatifs mais les risques doivent √™tre g√©r√©s.",
                "metadata": {"category": "strategy", "importance": "high"}
            },
            {
                "id": "biz_002",
                "content": "Les cas d'usage d'IA les plus rentables incluent l'automatisation des processus, l'analyse pr√©dictive, la personnalisation client et l'optimisation op√©rationnelle.",
                "metadata": {"category": "use_cases", "importance": "medium"}
            }
        ]
        
        # Ajouter les documents aux collections
        self._add_documents_to_collection("ai_knowledge", ai_documents)
        self._add_documents_to_collection("technical_docs", tech_documents) 
        self._add_documents_to_collection("business_context", business_documents)
        
        print("üìö Base de connaissances peupl√©e avec les documents initiaux")
    
    def _add_documents_to_collection(self, collection_name: str, documents: List[Dict]):
        """Ajouter des documents √† une collection"""
        collection = self.knowledge_collections[collection_name]
        
        # Pr√©parer les donn√©es
        ids = [doc["id"] for doc in documents]
        contents = [doc["content"] for doc in documents]
        metadatas = [doc["metadata"] for doc in documents]
        
        # G√©n√©rer les embeddings
        embeddings = self.embedding_model.encode(contents).tolist()
        
        # Ajouter √† la collection
        collection.add(
            ids=ids,
            documents=contents,
            embeddings=embeddings,
            metadatas=metadatas
        )
        
        print(f"   üìÑ {len(documents)} documents ajout√©s √† {collection_name}")
    
    async def process_rag_query(self, query: str, collections: Optional[List[str]] = None) -> Dict[str, Any]:
        """Traiter une requ√™te avec RAG"""
        print(f"\nüîç RAG QUERY: {query}")
        print("=" * 60)
        
        # S√©lectionner les collections √† interroger
        if collections is None:
            collections = list(self.knowledge_collections.keys())
        
        # Phase 1: R√©cup√©ration (Retrieval)
        retrieval_results = await self._retrieve_relevant_knowledge(query, collections)
        
        # Phase 2: Reranking et filtrage
        filtered_results = self._rerank_and_filter_results(query, retrieval_results)
        
        # Phase 3: Construction du contexte
        context = self._build_rag_context(filtered_results)
        
        # Phase 4: G√©n√©ration augment√©e
        generated_response = await self._generate_with_context(query, context)
        
        # Phase 5: V√©rification et validation
        verification = await self._verify_response_accuracy(query, generated_response, filtered_results)
        
        # Compiler les r√©sultats
        result = {
            "query": query,
            "collections_searched": collections,
            "retrieved_documents": len(retrieval_results),
            "filtered_documents": len(filtered_results),
            "context_length": len(context),
            "generated_response": generated_response,
            "source_documents": filtered_results,
            "verification": verification,
            "rag_quality_score": self._calculate_rag_quality(retrieval_results, generated_response, verification),
            "success": verification.get("factual_accuracy", 0) > 0.7
        }
        
        print(f"‚úÖ RAG traitement termin√©:")
        print(f"   ‚Ä¢ Documents r√©cup√©r√©s: {result['retrieved_documents']}")
        print(f"   ‚Ä¢ Documents pertinents: {result['filtered_documents']}")
        print(f"   ‚Ä¢ Score qualit√© RAG: {result['rag_quality_score']:.2f}")
        print(f"   ‚Ä¢ Exactitude factuelle: {verification.get('factual_accuracy', 0):.2f}")
        
        return result
    
    async def _retrieve_relevant_knowledge(self, query: str, collections: List[str]) -> List[Dict]:
        """R√©cup√©rer les connaissances pertinentes"""
        all_results = []
        
        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = self.embedding_model.encode([query])[0].tolist()
        
        print(f"üîé Recherche dans {len(collections)} collections...")
        
        for collection_name in collections:
            collection = self.knowledge_collections[collection_name]
            
            # Rechercher dans la collection
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=self.retrieval_config["top_k"],
                include=["documents", "metadatas", "distances"]
            )
            
            # Traiter les r√©sultats
            if results["documents"] and len(results["documents"]) > 0:
                for i, (doc, metadata, distance) in enumerate(zip(
                    results["documents"][0],
                    results["metadatas"][0], 
                    results["distances"][0]
                )):
                    similarity = 1 - distance  # Convertir distance en similarit√©
                    
                    if similarity >= self.retrieval_config["similarity_threshold"]:
                        all_results.append({
                            "collection": collection_name,
                            "document": doc,
                            "metadata": metadata,
                            "similarity": similarity,
                            "distance": distance,
                            "rank": i + 1
                        })
            
            print(f"   üìö {collection_name}: {len([r for r in all_results if r['collection'] == collection_name])} documents pertinents")
        
        # Trier par similarit√© globale
        all_results.sort(key=lambda x: x["similarity"], reverse=True)
        
        return all_results
    
    def _rerank_and_filter_results(self, query: str, results: List[Dict]) -> List[Dict]:
        """Reranker et filtrer les r√©sultats"""
        if not self.retrieval_config["rerank_results"]:
            return results
        
        print("üîÑ Reranking et filtrage des r√©sultats...")
        
        # Filtrer par seuil de similarit√©
        filtered = [r for r in results if r["similarity"] >= self.retrieval_config["similarity_threshold"]]
        
        # Reranking bas√© sur plusieurs crit√®res
        for result in filtered:
            rerank_score = result["similarity"]
            
            # Bonus pour l'importance
            importance = result["metadata"].get("importance", "medium")
            if importance == "high":
                rerank_score *= 1.2
            elif importance == "low":
                rerank_score *= 0.8
            
            # Bonus pour la correspondance de cat√©gorie
            query_lower = query.lower()
            category = result["metadata"].get("category", "")
            if category and category in query_lower:
                rerank_score *= 1.15
            
            result["rerank_score"] = rerank_score
        
        # Re-trier par score de reranking
        filtered.sort(key=lambda x: x["rerank_score"], reverse=True)
        
        # Limiter le nombre de r√©sultats
        max_results = min(self.retrieval_config["top_k"], len(filtered))
        final_results = filtered[:max_results]
        
        print(f"   üéØ {len(final_results)} documents apr√®s reranking")
        
        return final_results
    
    def _build_rag_context(self, filtered_results: List[Dict]) -> str:
        """Construire le contexte RAG"""
        if not filtered_results:
            return "Aucune information pertinente trouv√©e dans la base de connaissances."
        
        context_parts = ["Informations pertinentes de la base de connaissances:\n"]
        current_length = len(context_parts[0])
        
        for i, result in enumerate(filtered_results, 1):
            doc_context = f"{i}. [{result['collection']}] {result['document']}\n"
            doc_context += f"   Pertinence: {result['similarity']:.2f}\n\n"
            
            # V√©rifier la limite de longueur
            if current_length + len(doc_context) > self.retrieval_config["max_context_length"]:
                break
            
            context_parts.append(doc_context)
            current_length += len(doc_context)
        
        context = "".join(context_parts)
        
        print(f"üìù Contexte construit: {len(context)} caract√®res")
        
        return context
    
    async def _generate_with_context(self, query: str, context: str) -> str:
        """G√©n√©rer une r√©ponse avec le contexte RAG"""
        system_prompt = """Tu es un assistant IA expert qui utilise des informations de base de connaissances pour r√©pondre aux questions.

Instructions importantes:
- Utilise UNIQUEMENT les informations fournies dans le contexte
- Si l'information n'est pas dans le contexte, dis-le clairement
- Cite les sources quand tu utilises des informations sp√©cifiques
- Sois pr√©cis et factuel
- Indique ton niveau de confiance dans la r√©ponse"""
        
        user_prompt = f"""Contexte de la base de connaissances:
{context}

Question de l'utilisateur: {query}

R√©ponds en utilisant les informations du contexte ci-dessus. Si certaines informations manquent, indique-le clairement."""
        
        # Simulation de g√©n√©ration LLM avec contexte
        # En production: appel r√©el √† l'API OpenAI avec le contexte
        generated_response = f"""Bas√© sur les informations de la base de connaissances, voici ma r√©ponse √† votre question "{query}":

D'apr√®s les documents consult√©s, {query.lower()} implique plusieurs aspects importants. Les informations disponibles indiquent que ce sujet est bien document√© dans notre base de connaissances.

Les points cl√©s identifi√©s sont:
- Les concepts fondamentaux sont clairement d√©finis
- Les applications pratiques sont document√©es
- Les consid√©rations techniques sont d√©taill√©es

Cette r√©ponse est bas√©e sur {len(context.split('.'))} √©l√©ments d'information de la base de connaissances, avec un niveau de confiance √©lev√© pour les aspects couverts.

Note: Cette r√©ponse utilise exclusivement les informations disponibles dans la base de connaissances consult√©e."""
        
        return generated_response
    
    async def _verify_response_accuracy(self, query: str, response: str, source_docs: List[Dict]) -> Dict[str, Any]:
        """V√©rifier l'exactitude de la r√©ponse"""
        verification = {
            "factual_accuracy": 0.0,
            "source_alignment": 0.0,
            "completeness": 0.0,
            "confidence_level": 0.0,
            "verification_notes": []
        }
        
        # V√©rifier l'alignement avec les sources
        if source_docs:
            # Calculer l'alignement s√©mantique
            response_words = set(response.lower().split())
            source_words = set()
            
            for doc in source_docs:
                doc_words = set(doc["document"].lower().split())
                source_words.update(doc_words)
            
            if source_words:
                alignment = len(response_words.intersection(source_words)) / len(response_words.union(source_words))
                verification["source_alignment"] = alignment
            
            # √âvaluer l'exactitude factuelle (bas√©e sur la similarit√© avec les sources)
            avg_similarity = sum(doc["similarity"] for doc in source_docs) / len(source_docs)
            verification["factual_accuracy"] = avg_similarity
            
            verification["verification_notes"].append(f"Alignement avec {len(source_docs)} sources")
        
        # √âvaluer la compl√©tude
        query_concepts = len(query.split())
        response_concepts = len(response.split())
        
        if query_concepts > 0:
            completeness = min(1.0, response_concepts / (query_concepts * 10))  # Heuristique
            verification["completeness"] = completeness
        
        # Niveau de confiance global
        verification["confidence_level"] = (
            verification["factual_accuracy"] * 0.4 +
            verification["source_alignment"] * 0.3 +
            verification["completeness"] * 0.3
        )
        
        if verification["confidence_level"] > 0.8:
            verification["verification_notes"].append("Haute confiance dans la r√©ponse")
        elif verification["confidence_level"] > 0.6:
            verification["verification_notes"].append("Confiance mod√©r√©e")
        else:
            verification["verification_notes"].append("Confiance faible - v√©rification recommand√©e")
        
        return verification
    
    def _calculate_rag_quality(self, retrieval_results: List[Dict], response: str, verification: Dict) -> float:
        """Calculer la qualit√© globale du processus RAG"""
        quality_factors = {
            "retrieval_quality": 0.0,
            "context_relevance": 0.0,
            "generation_quality": 0.0,
            "factual_accuracy": verification.get("factual_accuracy", 0.0)
        }
        
        # Qualit√© de r√©cup√©ration
        if retrieval_results:
            avg_similarity = sum(r["similarity"] for r in retrieval_results) / len(retrieval_results)
            quality_factors["retrieval_quality"] = avg_similarity
        
        # Pertinence du contexte
        high_similarity_docs = [r for r in retrieval_results if r["similarity"] > 0.8]
        if retrieval_results:
            quality_factors["context_relevance"] = len(high_similarity_docs) / len(retrieval_results)
        
        # Qualit√© de g√©n√©ration (bas√©e sur la longueur et structure de la r√©ponse)
        if response:
            # Heuristiques simples pour √©valuer la qualit√©
            word_count = len(response.split())
            has_structure = any(marker in response for marker in [":", "-", "‚Ä¢", "1.", "2."])
            
            generation_quality = 0.5  # Score de base
            if word_count >= 50:
                generation_quality += 0.2
            if word_count >= 100:
                generation_quality += 0.1
            if has_structure:
                generation_quality += 0.2
            
            quality_factors["generation_quality"] = min(1.0, generation_quality)
        
        # Score global pond√©r√©
        overall_quality = (
            quality_factors["retrieval_quality"] * 0.25 +
            quality_factors["context_relevance"] * 0.25 +
            quality_factors["generation_quality"] * 0.25 +
            quality_factors["factual_accuracy"] * 0.25
        )
        
        return overall_quality
    
    async def add_knowledge_document(self, collection_name: str, document: Dict[str, Any]) -> bool:
        """Ajouter un nouveau document √† la base de connaissances"""
        try:
            if collection_name not in self.knowledge_collections:
                print(f"‚ùå Collection {collection_name} n'existe pas")
                return False
            
            # Ajouter le document
            self._add_documents_to_collection(collection_name, [document])
            
            print(f"‚úÖ Document ajout√© √† {collection_name}: {document['id']}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'ajout du document: {e}")
            return False
    
    async def demonstrate_rag_capabilities(self):
        """D√©monstration des capacit√©s RAG"""
        print(f"\nüé¨ D√âMONSTRATION: Pattern RAG Agent")
        print("=" * 80)
        
        demo_queries = [
            "Qu'est-ce que l'intelligence artificielle ?",
            "Comment fonctionnent les agents IA ?", 
            "Quels sont les avantages du RAG ?",
            "Quel langage de programmation utiliser pour l'IA ?",
            "Comment impl√©menter l'IA en entreprise ?",
            "Que sont les embeddings ?"
        ]
        
        results = []
        
        for i, query in enumerate(demo_queries, 1):
            print(f"\n--- Requ√™te RAG {i}/{len(demo_queries)} ---")
            print(f"‚ùì Question: {query}")
            
            result = await self.process_rag_query(query)
            results.append(result)
            
            print(f"\nüìä R√©sultats pour la requ√™te {i}:")
            print(f"   ‚Ä¢ Documents trouv√©s: {result['retrieved_documents']}")
            print(f"   ‚Ä¢ Documents pertinents: {result['filtered_documents']}")
            print(f"   ‚Ä¢ Exactitude factuelle: {result['verification']['factual_accuracy']:.2f}")
            print(f"   ‚Ä¢ Score qualit√© RAG: {result['rag_quality_score']:.2f}")
            print(f"   ‚Ä¢ Succ√®s: {'Oui' if result['success'] else 'Non'}")
            print(f"   ‚Ä¢ R√©ponse: {result['generated_response'][:150]}...")
        
        # Statistiques globales
        print(f"\nüìà STATISTIQUES RAG")
        print("=" * 60)
        
        total_queries = len(results)
        successful_queries = sum(1 for r in results if r["success"])
        avg_quality = sum(r["rag_quality_score"] for r in results) / total_queries
        avg_accuracy = sum(r["verification"]["factual_accuracy"] for r in results) / total_queries
        total_docs_retrieved = sum(r["retrieved_documents"] for r in results)
        
        print(f"üìä Total requ√™tes: {total_queries}")
        print(f"‚úÖ Requ√™tes r√©ussies: {successful_queries} ({successful_queries/total_queries:.1%})")
        print(f"üéØ Qualit√© RAG moyenne: {avg_quality:.2f}")
        print(f"üìö Exactitude factuelle moyenne: {avg_accuracy:.2f}")
        print(f"üîç Total documents r√©cup√©r√©s: {total_docs_retrieved}")
        print(f"üíæ Collections actives: {len(self.knowledge_collections)}")
        
        return results
```

I'll continue with the remaining TODOs in my next response to complete the comprehensive guide for Module 3.